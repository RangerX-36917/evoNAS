\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry}

\begin{document}
	\begin{center}
    
    	% MAKE SURE YOU TAKE OUT THE SQUARE BRACKETS
    
		\LARGE{\textbf{Evolutionary Neural Architecture Search for Image Classification}} \\
        \vspace{1em}
        \Large{Project Proposal} \\
        \vspace{1em}
        \normalsize\textbf{ Bowen Zheng   }  
        \normalsize\textbf{ Shijie Chen   } 
        \normalsize\textbf{ Shuxin Wang  } 
        
        \vspace{1em}
        \normalsize{Advisor: Hisao Ishibuchi} \\
        \vspace{1em}
        \normalsize{Southern University of Science and Technology} 
	\end{center}
    \begin{normalsize}
    
    	
      
		\section{Background \& Rationale}
        
      The great leap of computing resources in the past few decades made it possible to fully utilize the potential of neural networks. In recent years neural networks outperformed traditional methods in many fields of research, especially image classification. However, the state-of-the-art architectures are carefully designed and tuned by researchers for a specific problem. Therefore, people start to think about automate the design of neural networks in the hope of finding the best-performing network architecture efficiently.

      Neural architecture search is a research field focusing on automating the design of neural networks. Currently there are a few popular approaches, including reinforcement learning, bayesian optimization, tree-based searching and genetic-based evolutionary algorithms. 

      This project focuses on NAS for image classification problems. The reason is that this area is well explored and there exists many high-performance hand-crafted neural architectures. They provide a good guidance and target to our project. In addition, neural networks for image classification are mostly built upon basic cells including convolution, polling, normalization, and activation layers. This helps to shrink our search space.
    
    \section{Problem Definition}
      Neural Architecture Search (NAS) refers to the process of automatically designing artificial neural network architectures \cite{elsken2018neural}. Research topics in NAS are generally categorized into three categories:
      \begin{enumerate}
        \item \textbf{\emph{Search Space}}
         The search space of NAS denotes the space in which the NAS algorithm tries to find a neural network architecture.
        \item \textbf{\emph{Search Strategy}} The search strategy of NAS denotes the search algorithm that is used to explore the \emph{search space}.
        \item \textbf{\emph{Performance Estimation Strategy}} NAS algorithms will generate a large quantity of neural network architectures during search process. We need an efficient way to estimate the performance of each architecture to cut the demand for computational resource. 
      \end{enumerate}

   
    \section{Objective}

    The objective of this project is to propose a novel evolutionary approach for neural architecture search targeted at image classification. The specific objectives are as follows:
    \begin{enumerate}
      \item Investigate existing evolutionary neural architecture search algorithms.
      \item Propose effective genetic operators for evolution of neural networks. This includes \emph{crossover}, \emph{mutation} and \emph{inheritance}.
      \item Find an efficient estimate approximation of neural networks to cut computational cost.
      \item Identify an effective population selection strategy.
      \item Compare performance against hand-crafted networks as well as other NAS algorithms.
    \end{enumerate}

    \section{Related Works}
    A lot of research works have been done in each of the three categories in NAS. Some proposed algorithms can design architectures that is on par of or even more capable than state-of-the-art hand-crafted networks.
    
    \subsection{Search Space}
    
    The search space of NAS determines the possible architectures a NAS algorithm can find.

    The simplest search space is the simple multiple-layer structure, in which a neural network $A$ is composed of multiple layers $L_i$ connected to the neighboring layers. In this case, the search space can be described by (1) The maximum number of layers (2) The type and dimension of each layer and their hyper-parameters. 

    In more recent studies, some researchers use a cell based search space in which the possible architecture of cells are explored. A cell is nothing but a smaller neural network. The entire neural network is constructed by connecting several pre-defined cells. A cell has less layers but allows more complex architectures like skip connections between any layers. The cell could be some hand-crafted neural networks that have already been proofed effective. The search space is therefore decreased to the possible arrangements of cells.

    In contrast to the above direction, some researchers also tried to search for effective cells and connect them at last in a predefined manner. The search space is greatly decreased in that each cell is comparably small. This method can also be easily transferred to other datasets since the structure of cells are not fixed.

    Recently, some researchers managed to optimize the overall architecture as well as the cells at the same time and obtained state-of-the-art result.
    
    \subsection{Search Strategy}
    
    Many different search strategies can be applied to explore the search space discussed above. These methods includes bayesian optimization, evolutionary methods, reinforcement learning and gradient-based methods. 

    Evolutionary algorithms has been used to evolve neural networks since 1989. Earlier works use genetic algorithms to both optimize the structure of neural networks and train the networks. However, with the birth of back-propagation (BP), recent works of neural-evolution use genetic algorithms only for optimizing neural architectures and use BP to train the networks. In the context of NAS, the individuals in the genetic algorithm are neural network architectures and the genetic operations (crossover and mutation) are used to alter the architecture by adding/removing layers or change connectivity of nodes.

    Genetic algorithms shows their diversity in how they sample parents, generate offspring and update population. Some work choose parents from a preto-optimal front while others use tournament selection. When generating offsprings, some algorithms randomly initialize weight of child networks. In comparison, Lamarckian inheritance is used in [XXX] so that child networks could inherit weight of its parent and the training cost is reduced. To update population, some algorithms abandon least capable individuals while some delete the oldest individuals. 
    
    There are other methods that are used to implement NAS, including bayesian optimization, reinforcement learning ,tree-based search and gradient-based methods. We don't discuss them here since we use evolutionary algorithms in our project.
    \subsection{Performance Estimation Strategy}
    One important issue in neural architecture search is the estimation of neural network performance. This is critical in the population update policy of evolutionary algorithms. 

    The simplest way to estimate performance is to train every searched network from scratch and test the desired performance metric, e.g. accuracy on validation set. However, the training of neural network is very time and computation consuming. 
    
    An alternative is to estimate performance on lower fidelities. More specifically, to train the network for shorter period of time or on some subset of the dataset. However, the estimate must ensure the result ranking of different networks must be the same as that of complete training. That is to say, there exist a trade-off between computational load and estimation fidelity.

    Another approach to estimate performance is based on learning curve extrapolation\cite{domhan2015speeding}. This method accelerate estimation by stop poor performance networks at the early state of training based on statistical patterns of learning curves. Other researchers propose ways to predict neural network performance based on architectural and cell properties\cite{liu2018progressive}. 
    \section{Methodology}

    There are several parts that needs to be done to complete this project.
    \begin{enumerate}
      \item Set the search space.
      \item Design evolutionary operators and population update policy.
      \item Design performance estimator for neural networks according to problem size and computational resources.
    \end{enumerate}
      
    
      
    \noindent You can create bullets by doing:

  \section{System Design}
  
  \section{Timeline}
\end{normalsize}
     

\bibliographystyle{ieeetr}
\bibliography{ref}

\end{document}